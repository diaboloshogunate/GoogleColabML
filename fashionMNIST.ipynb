{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQLZR5eWwK7wqx18wMGMdB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diaboloshogunate/GoogleColabML/blob/main/fashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "5G7ZTwesYf4p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "momentum = 0\n",
        "epochs = 2\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "layers = [nn.Linear(28*28, 1024), nn.ReLU(), nn.Linear(1024, 10)]\n",
        "nn.Sequential(*layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx_riMXQdl1S",
        "outputId": "dd755643-1cbc-438d-c09d-85697ca5a1eb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=1024, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=1024, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets\n",
        "trainset = torchvision.datasets.FashionMNIST('./data', download=True, train=True, transform=ToTensor())\n",
        "testset = torchvision.datasets.FashionMNIST('./data', download=True, train=False, transform=ToTensor())"
      ],
      "metadata": {
        "id": "O7iU8YplZuH1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "1AbeSX1RbQ3B"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
      ],
      "metadata": {
        "id": "IAGTm-sXbhKr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in testloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaoXntend240",
        "outputId": "a2aaae96-bbd2-4088-8e80-0c4440601a60"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]:  torch.Size([4, 1, 28, 28])\n",
            "Shape of y:  torch.Size([4]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqschsvYcko7",
        "outputId": "9166bc97-d759-4db2-8686-b464aa009f24"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, stack):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = stack\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork(stack).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODYp2hTPfJ6B",
        "outputId": "345c889c-132b-4b50-8db1-3156e4769c9a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "-vydS_HwnztM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(testloader, model, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_br2KyroVoc",
        "outputId": "b6ce5258-18ca-4273-fca3-59815861e1c0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.279865  [    0/60000]\n",
            "loss: 2.354511  [  400/60000]\n",
            "loss: 2.084656  [  800/60000]\n",
            "loss: 1.998106  [ 1200/60000]\n",
            "loss: 1.925862  [ 1600/60000]\n",
            "loss: 1.798301  [ 2000/60000]\n",
            "loss: 1.648045  [ 2400/60000]\n",
            "loss: 1.798101  [ 2800/60000]\n",
            "loss: 1.808859  [ 3200/60000]\n",
            "loss: 1.615044  [ 3600/60000]\n",
            "loss: 1.423078  [ 4000/60000]\n",
            "loss: 1.444520  [ 4400/60000]\n",
            "loss: 1.825121  [ 4800/60000]\n",
            "loss: 1.321168  [ 5200/60000]\n",
            "loss: 1.524265  [ 5600/60000]\n",
            "loss: 1.627309  [ 6000/60000]\n",
            "loss: 1.393851  [ 6400/60000]\n",
            "loss: 1.548409  [ 6800/60000]\n",
            "loss: 1.302371  [ 7200/60000]\n",
            "loss: 1.905776  [ 7600/60000]\n",
            "loss: 1.784188  [ 8000/60000]\n",
            "loss: 0.841141  [ 8400/60000]\n",
            "loss: 1.373933  [ 8800/60000]\n",
            "loss: 1.378284  [ 9200/60000]\n",
            "loss: 1.354561  [ 9600/60000]\n",
            "loss: 0.744157  [10000/60000]\n",
            "loss: 0.658295  [10400/60000]\n",
            "loss: 0.989763  [10800/60000]\n",
            "loss: 1.366536  [11200/60000]\n",
            "loss: 1.038715  [11600/60000]\n",
            "loss: 1.192597  [12000/60000]\n",
            "loss: 1.664562  [12400/60000]\n",
            "loss: 0.812769  [12800/60000]\n",
            "loss: 1.272579  [13200/60000]\n",
            "loss: 0.956575  [13600/60000]\n",
            "loss: 0.859540  [14000/60000]\n",
            "loss: 0.610889  [14400/60000]\n",
            "loss: 0.610117  [14800/60000]\n",
            "loss: 0.454009  [15200/60000]\n",
            "loss: 0.412038  [15600/60000]\n",
            "loss: 1.394543  [16000/60000]\n",
            "loss: 0.630009  [16400/60000]\n",
            "loss: 0.838226  [16800/60000]\n",
            "loss: 0.575038  [17200/60000]\n",
            "loss: 1.507704  [17600/60000]\n",
            "loss: 0.645886  [18000/60000]\n",
            "loss: 1.205873  [18400/60000]\n",
            "loss: 0.679245  [18800/60000]\n",
            "loss: 1.029478  [19200/60000]\n",
            "loss: 0.482239  [19600/60000]\n",
            "loss: 1.162360  [20000/60000]\n",
            "loss: 1.735681  [20400/60000]\n",
            "loss: 0.938956  [20800/60000]\n",
            "loss: 1.237731  [21200/60000]\n",
            "loss: 0.690602  [21600/60000]\n",
            "loss: 0.792460  [22000/60000]\n",
            "loss: 0.923596  [22400/60000]\n",
            "loss: 0.765965  [22800/60000]\n",
            "loss: 0.831505  [23200/60000]\n",
            "loss: 0.702079  [23600/60000]\n",
            "loss: 0.264844  [24000/60000]\n",
            "loss: 0.709756  [24400/60000]\n",
            "loss: 1.344239  [24800/60000]\n",
            "loss: 0.721903  [25200/60000]\n",
            "loss: 0.801308  [25600/60000]\n",
            "loss: 1.020330  [26000/60000]\n",
            "loss: 0.660741  [26400/60000]\n",
            "loss: 0.625545  [26800/60000]\n",
            "loss: 0.916381  [27200/60000]\n",
            "loss: 0.662478  [27600/60000]\n",
            "loss: 0.542517  [28000/60000]\n",
            "loss: 0.708748  [28400/60000]\n",
            "loss: 0.835697  [28800/60000]\n",
            "loss: 0.911569  [29200/60000]\n",
            "loss: 0.260681  [29600/60000]\n",
            "loss: 0.854088  [30000/60000]\n",
            "loss: 0.907655  [30400/60000]\n",
            "loss: 0.403336  [30800/60000]\n",
            "loss: 0.280812  [31200/60000]\n",
            "loss: 0.828409  [31600/60000]\n",
            "loss: 0.958009  [32000/60000]\n",
            "loss: 0.631111  [32400/60000]\n",
            "loss: 0.505800  [32800/60000]\n",
            "loss: 0.525469  [33200/60000]\n",
            "loss: 0.921173  [33600/60000]\n",
            "loss: 0.700121  [34000/60000]\n",
            "loss: 0.673239  [34400/60000]\n",
            "loss: 0.255602  [34800/60000]\n",
            "loss: 0.408677  [35200/60000]\n",
            "loss: 1.169308  [35600/60000]\n",
            "loss: 0.717243  [36000/60000]\n",
            "loss: 0.649157  [36400/60000]\n",
            "loss: 0.790606  [36800/60000]\n",
            "loss: 0.269534  [37200/60000]\n",
            "loss: 0.427429  [37600/60000]\n",
            "loss: 0.993706  [38000/60000]\n",
            "loss: 1.171330  [38400/60000]\n",
            "loss: 1.062431  [38800/60000]\n",
            "loss: 0.538592  [39200/60000]\n",
            "loss: 0.494907  [39600/60000]\n",
            "loss: 0.409559  [40000/60000]\n",
            "loss: 0.868538  [40400/60000]\n",
            "loss: 0.712350  [40800/60000]\n",
            "loss: 0.850800  [41200/60000]\n",
            "loss: 0.471260  [41600/60000]\n",
            "loss: 1.199604  [42000/60000]\n",
            "loss: 1.007548  [42400/60000]\n",
            "loss: 1.035442  [42800/60000]\n",
            "loss: 0.586904  [43200/60000]\n",
            "loss: 0.944940  [43600/60000]\n",
            "loss: 1.622897  [44000/60000]\n",
            "loss: 0.337922  [44400/60000]\n",
            "loss: 0.206895  [44800/60000]\n",
            "loss: 0.183734  [45200/60000]\n",
            "loss: 0.568487  [45600/60000]\n",
            "loss: 0.318541  [46000/60000]\n",
            "loss: 1.115224  [46400/60000]\n",
            "loss: 0.827057  [46800/60000]\n",
            "loss: 1.217278  [47200/60000]\n",
            "loss: 0.486303  [47600/60000]\n",
            "loss: 0.271766  [48000/60000]\n",
            "loss: 0.853103  [48400/60000]\n",
            "loss: 0.547506  [48800/60000]\n",
            "loss: 0.360113  [49200/60000]\n",
            "loss: 0.313882  [49600/60000]\n",
            "loss: 0.546012  [50000/60000]\n",
            "loss: 0.612901  [50400/60000]\n",
            "loss: 0.280466  [50800/60000]\n",
            "loss: 0.375842  [51200/60000]\n",
            "loss: 0.760262  [51600/60000]\n",
            "loss: 0.617781  [52000/60000]\n",
            "loss: 0.422298  [52400/60000]\n",
            "loss: 1.797354  [52800/60000]\n",
            "loss: 0.223995  [53200/60000]\n",
            "loss: 0.497349  [53600/60000]\n",
            "loss: 0.314815  [54000/60000]\n",
            "loss: 0.775238  [54400/60000]\n",
            "loss: 1.815201  [54800/60000]\n",
            "loss: 1.076935  [55200/60000]\n",
            "loss: 0.715912  [55600/60000]\n",
            "loss: 0.705493  [56000/60000]\n",
            "loss: 0.823854  [56400/60000]\n",
            "loss: 0.465989  [56800/60000]\n",
            "loss: 0.162831  [57200/60000]\n",
            "loss: 0.215163  [57600/60000]\n",
            "loss: 0.214011  [58000/60000]\n",
            "loss: 0.279259  [58400/60000]\n",
            "loss: 0.442113  [58800/60000]\n",
            "loss: 0.805706  [59200/60000]\n",
            "loss: 0.795392  [59600/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 0.637738 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.103229  [    0/60000]\n",
            "loss: 0.378968  [  400/60000]\n",
            "loss: 0.376715  [  800/60000]\n",
            "loss: 0.693442  [ 1200/60000]\n",
            "loss: 0.975556  [ 1600/60000]\n",
            "loss: 0.313135  [ 2000/60000]\n",
            "loss: 0.365244  [ 2400/60000]\n",
            "loss: 0.743698  [ 2800/60000]\n",
            "loss: 0.556883  [ 3200/60000]\n",
            "loss: 0.248549  [ 3600/60000]\n",
            "loss: 0.392040  [ 4000/60000]\n",
            "loss: 0.339079  [ 4400/60000]\n",
            "loss: 0.705978  [ 4800/60000]\n",
            "loss: 0.395752  [ 5200/60000]\n",
            "loss: 1.203377  [ 5600/60000]\n",
            "loss: 0.775598  [ 6000/60000]\n",
            "loss: 0.369012  [ 6400/60000]\n",
            "loss: 0.669216  [ 6800/60000]\n",
            "loss: 0.568571  [ 7200/60000]\n",
            "loss: 0.035028  [ 7600/60000]\n",
            "loss: 0.614052  [ 8000/60000]\n",
            "loss: 1.910348  [ 8400/60000]\n",
            "loss: 0.684103  [ 8800/60000]\n",
            "loss: 0.402708  [ 9200/60000]\n",
            "loss: 0.249292  [ 9600/60000]\n",
            "loss: 0.463618  [10000/60000]\n",
            "loss: 0.828737  [10400/60000]\n",
            "loss: 0.342390  [10800/60000]\n",
            "loss: 1.060650  [11200/60000]\n",
            "loss: 0.269657  [11600/60000]\n",
            "loss: 0.685513  [12000/60000]\n",
            "loss: 0.304462  [12400/60000]\n",
            "loss: 0.806081  [12800/60000]\n",
            "loss: 0.424582  [13200/60000]\n",
            "loss: 0.735406  [13600/60000]\n",
            "loss: 0.144579  [14000/60000]\n",
            "loss: 1.075534  [14400/60000]\n",
            "loss: 0.502145  [14800/60000]\n",
            "loss: 0.316051  [15200/60000]\n",
            "loss: 0.889213  [15600/60000]\n",
            "loss: 0.903247  [16000/60000]\n",
            "loss: 0.635565  [16400/60000]\n",
            "loss: 0.312293  [16800/60000]\n",
            "loss: 0.175928  [17200/60000]\n",
            "loss: 0.391252  [17600/60000]\n",
            "loss: 0.410807  [18000/60000]\n",
            "loss: 0.949392  [18400/60000]\n",
            "loss: 0.138943  [18800/60000]\n",
            "loss: 0.802268  [19200/60000]\n",
            "loss: 0.192254  [19600/60000]\n",
            "loss: 0.486235  [20000/60000]\n",
            "loss: 0.532964  [20400/60000]\n",
            "loss: 1.015783  [20800/60000]\n",
            "loss: 0.546236  [21200/60000]\n",
            "loss: 0.424499  [21600/60000]\n",
            "loss: 0.238390  [22000/60000]\n",
            "loss: 0.320053  [22400/60000]\n",
            "loss: 0.858986  [22800/60000]\n",
            "loss: 0.324402  [23200/60000]\n",
            "loss: 0.797526  [23600/60000]\n",
            "loss: 0.547686  [24000/60000]\n",
            "loss: 0.930902  [24400/60000]\n",
            "loss: 0.547952  [24800/60000]\n",
            "loss: 0.449976  [25200/60000]\n",
            "loss: 0.761167  [25600/60000]\n",
            "loss: 0.572342  [26000/60000]\n",
            "loss: 0.023097  [26400/60000]\n",
            "loss: 1.394865  [26800/60000]\n",
            "loss: 0.494827  [27200/60000]\n",
            "loss: 0.828511  [27600/60000]\n",
            "loss: 0.191221  [28000/60000]\n",
            "loss: 0.465251  [28400/60000]\n",
            "loss: 0.642593  [28800/60000]\n",
            "loss: 0.437073  [29200/60000]\n",
            "loss: 0.267149  [29600/60000]\n",
            "loss: 0.279306  [30000/60000]\n",
            "loss: 1.311391  [30400/60000]\n",
            "loss: 1.344229  [30800/60000]\n",
            "loss: 0.851992  [31200/60000]\n",
            "loss: 0.529274  [31600/60000]\n",
            "loss: 0.248044  [32000/60000]\n",
            "loss: 0.766657  [32400/60000]\n",
            "loss: 0.139129  [32800/60000]\n",
            "loss: 0.869815  [33200/60000]\n",
            "loss: 0.860059  [33600/60000]\n",
            "loss: 0.941432  [34000/60000]\n",
            "loss: 0.494879  [34400/60000]\n",
            "loss: 0.892799  [34800/60000]\n",
            "loss: 0.763070  [35200/60000]\n",
            "loss: 0.926579  [35600/60000]\n",
            "loss: 0.275936  [36000/60000]\n",
            "loss: 0.316874  [36400/60000]\n",
            "loss: 0.390902  [36800/60000]\n",
            "loss: 0.753535  [37200/60000]\n",
            "loss: 0.471915  [37600/60000]\n",
            "loss: 1.472516  [38000/60000]\n",
            "loss: 0.197209  [38400/60000]\n",
            "loss: 0.432085  [38800/60000]\n",
            "loss: 0.127371  [39200/60000]\n",
            "loss: 0.449709  [39600/60000]\n",
            "loss: 0.161735  [40000/60000]\n",
            "loss: 0.397774  [40400/60000]\n",
            "loss: 1.040015  [40800/60000]\n",
            "loss: 0.705810  [41200/60000]\n",
            "loss: 0.267907  [41600/60000]\n",
            "loss: 0.110407  [42000/60000]\n",
            "loss: 0.648102  [42400/60000]\n",
            "loss: 0.079591  [42800/60000]\n",
            "loss: 0.552846  [43200/60000]\n",
            "loss: 0.882347  [43600/60000]\n",
            "loss: 0.469684  [44000/60000]\n",
            "loss: 0.100099  [44400/60000]\n",
            "loss: 0.563198  [44800/60000]\n",
            "loss: 0.189910  [45200/60000]\n",
            "loss: 1.619490  [45600/60000]\n",
            "loss: 0.186036  [46000/60000]\n",
            "loss: 0.509032  [46400/60000]\n",
            "loss: 0.288081  [46800/60000]\n",
            "loss: 0.307603  [47200/60000]\n",
            "loss: 0.477923  [47600/60000]\n",
            "loss: 0.487628  [48000/60000]\n",
            "loss: 0.710888  [48400/60000]\n",
            "loss: 1.204553  [48800/60000]\n",
            "loss: 0.170624  [49200/60000]\n",
            "loss: 0.497491  [49600/60000]\n",
            "loss: 0.114030  [50000/60000]\n",
            "loss: 0.239447  [50400/60000]\n",
            "loss: 0.174557  [50800/60000]\n",
            "loss: 0.837851  [51200/60000]\n",
            "loss: 0.112243  [51600/60000]\n",
            "loss: 0.304115  [52000/60000]\n",
            "loss: 0.493423  [52400/60000]\n",
            "loss: 0.329525  [52800/60000]\n",
            "loss: 0.663529  [53200/60000]\n",
            "loss: 1.282114  [53600/60000]\n",
            "loss: 0.134562  [54000/60000]\n",
            "loss: 0.057931  [54400/60000]\n",
            "loss: 0.363908  [54800/60000]\n",
            "loss: 1.744369  [55200/60000]\n",
            "loss: 0.444515  [55600/60000]\n",
            "loss: 1.231046  [56000/60000]\n",
            "loss: 0.358071  [56400/60000]\n",
            "loss: 0.897205  [56800/60000]\n",
            "loss: 0.158312  [57200/60000]\n",
            "loss: 0.096239  [57600/60000]\n",
            "loss: 1.183870  [58000/60000]\n",
            "loss: 0.614651  [58400/60000]\n",
            "loss: 0.423843  [58800/60000]\n",
            "loss: 0.344258  [59200/60000]\n",
            "loss: 0.606236  [59600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.540728 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}